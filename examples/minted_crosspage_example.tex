\section{Minted 跨页代码示例}

\subsection{基本minted环境（支持跨页）}

普通的minted环境现在默认支持跨页显示：

\begin{minted}{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd

def complex_data_analysis():
    """
    这是一个复杂的数据分析函数示例
    用于演示跨页代码显示功能
    """
    # 生成示例数据
    np.random.seed(42)
    n_samples = 1000
    n_features = 20
    
    # 创建特征矩阵
    X = np.random.randn(n_samples, n_features)
    noise = np.random.randn(n_samples) * 0.1
    
    # 创建目标变量
    true_coefficients = np.random.randn(n_features)
    y = X @ true_coefficients + noise
    
    # 分割数据集
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # 数据标准化
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 训练模型
    model = LinearRegression()
    model.fit(X_train_scaled, y_train)
    
    # 预测
    y_pred = model.predict(X_test_scaled)
    
    # 计算指标
    from sklearn.metrics import mean_squared_error, r2_score
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f"均方误差: {mse:.4f}")
    print(f"R² 分数: {r2:.4f}")
    
    # 可视化结果
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('实际值')
    plt.ylabel('预测值')
    plt.title('预测值 vs 实际值')
    
    plt.subplot(2, 2, 2)
    residuals = y_test - y_pred
    plt.scatter(y_pred, residuals, alpha=0.6)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('预测值')
    plt.ylabel('残差')
    plt.title('残差图')
    
    plt.subplot(2, 2, 3)
    plt.hist(residuals, bins=20, alpha=0.7, edgecolor='black')
    plt.xlabel('残差')
    plt.ylabel('频率')
    plt.title('残差分布')
    
    plt.subplot(2, 2, 4)
    feature_importance = np.abs(model.coef_)
    plt.bar(range(len(feature_importance)), feature_importance)
    plt.xlabel('特征索引')
    plt.ylabel('系数绝对值')
    plt.title('特征重要性')
    
    plt.tight_layout()
    plt.show()
    
    return {
        'model': model,
        'scaler': scaler,
        'mse': mse,
        'r2': r2,
        'coefficients': model.coef_,
        'predictions': y_pred
    }

# 运行分析
if __name__ == "__main__":
    results = complex_data_analysis()
    print("\n分析完成！")
    print(f"模型系数: {results['coefficients'][:5]}...")  # 只显示前5个系数
\end{minted}

\subsection{长代码环境（longcode）}

使用新定义的 \texttt{longcode} 环境：

\begin{longcode}[python]
class AdvancedNeuralNetwork:
    """
    高级神经网络实现
    支持多种激活函数和优化器
    """
    
    def __init__(self, layers, activation='relu', optimizer='adam'):
        self.layers = layers
        self.activation = activation
        self.optimizer = optimizer
        self.weights = []
        self.biases = []
        self.initialize_parameters()
        
    def initialize_parameters(self):
        """初始化网络参数"""
        for i in range(len(self.layers) - 1):
            # Xavier初始化
            w = np.random.randn(self.layers[i], self.layers[i+1]) * np.sqrt(2.0 / self.layers[i])
            b = np.zeros((1, self.layers[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def activation_function(self, x, derivative=False):
        """激活函数"""
        if self.activation == 'relu':
            if derivative:
                return (x > 0).astype(float)
            return np.maximum(0, x)
        elif self.activation == 'sigmoid':
            sig = 1 / (1 + np.exp(-np.clip(x, -500, 500)))
            if derivative:
                return sig * (1 - sig)
            return sig
        elif self.activation == 'tanh':
            if derivative:
                return 1 - np.tanh(x)**2
            return np.tanh(x)
    
    def forward_propagation(self, X):
        """前向传播"""
        self.layer_outputs = [X]
        current_input = X
        
        for i in range(len(self.weights)):
            z = np.dot(current_input, self.weights[i]) + self.biases[i]
            if i == len(self.weights) - 1:  # 输出层
                a = z  # 线性激活
            else:
                a = self.activation_function(z)
            self.layer_outputs.append(a)
            current_input = a
        
        return current_input
    
    def backward_propagation(self, X, y, learning_rate=0.01):
        """反向传播"""
        m = X.shape[0]
        
        # 计算输出层误差
        output_error = self.layer_outputs[-1] - y
        errors = [output_error]
        
        # 反向计算每一层的误差
        for i in range(len(self.weights) - 1, 0, -1):
            error = np.dot(errors[-1], self.weights[i].T) * \
                   self.activation_function(self.layer_outputs[i], derivative=True)
            errors.append(error)
        
        errors.reverse()
        
        # 更新权重和偏置
        for i in range(len(self.weights)):
            dw = np.dot(self.layer_outputs[i].T, errors[i]) / m
            db = np.sum(errors[i], axis=0, keepdims=True) / m
            
            self.weights[i] -= learning_rate * dw
            self.biases[i] -= learning_rate * db
    
    def train(self, X, y, epochs=1000, learning_rate=0.01, verbose=True):
        """训练神经网络"""
        losses = []
        
        for epoch in range(epochs):
            # 前向传播
            predictions = self.forward_propagation(X)
            
            # 计算损失
            loss = np.mean((predictions - y)**2)
            losses.append(loss)
            
            # 反向传播
            self.backward_propagation(X, y, learning_rate)
            
            if verbose and epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.6f}")
        
        return losses
    
    def predict(self, X):
        """预测"""
        return self.forward_propagation(X)

# 使用示例
if __name__ == "__main__":
    # 生成示例数据
    np.random.seed(42)
    X = np.random.randn(100, 2)
    y = (X[:, 0]**2 + X[:, 1]**2).reshape(-1, 1)
    
    # 创建和训练网络
    nn = AdvancedNeuralNetwork([2, 10, 10, 1], activation='relu')
    losses = nn.train(X, y, epochs=1000, learning_rate=0.01)
    
    # 评估
    predictions = nn.predict(X)
    mse = np.mean((predictions - y)**2)
    print(f"\n最终MSE: {mse:.6f}")
    
    # 绘制训练曲线
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plt.plot(losses)
    plt.title('训练损失')
    plt.xlabel('Epoch')
    plt.ylabel('MSE')
    
    plt.subplot(1, 2, 2)
    plt.scatter(y, predictions, alpha=0.6)
    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
    plt.xlabel('真实值')
    plt.ylabel('预测值')
    plt.title('预测结果')
    plt.tight_layout()
    plt.show()
\end{longcode}

\subsection{带标题的长代码框（longcodebox）}

\begin{longcodebox}[python]{深度学习模型实现}
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F

class ConvolutionalNeuralNetwork(nn.Module):
    """
    卷积神经网络实现
    适用于图像分类任务
    """
    
    def __init__(self, num_classes=10, input_channels=1):
        super(ConvolutionalNeuralNetwork, self).__init__()
        
        # 卷积层
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        
        # 批量归一化层
        self.bn1 = nn.BatchNorm2d(32)
        self.bn2 = nn.BatchNorm2d(64)
        self.bn3 = nn.BatchNorm2d(128)
        
        # Dropout层
        self.dropout = nn.Dropout(0.5)
        
        # 全连接层
        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # 假设输入图像大小为32x32
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, num_classes)
        
        # 池化层
        self.pool = nn.MaxPool2d(2, 2)
        
    def forward(self, x):
        # 第一个卷积块
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        
        # 第二个卷积块
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        
        # 第三个卷积块
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        
        # 展平
        x = x.view(-1, 128 * 4 * 4)
        
        # 全连接层
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        
        return x

class Trainer:
    """模型训练器"""
    
    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, 'min', patience=5, factor=0.5
        )
        
    def train_epoch(self, dataloader):
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            if batch_idx % 100 == 0:
                print(f'Train Batch: {batch_idx}, Loss: {loss.item():.6f}')
        
        accuracy = 100. * correct / total
        avg_loss = total_loss / len(dataloader)
        return avg_loss, accuracy
    
    def validate(self, dataloader):
        """验证模型"""
        self.model.eval()
        test_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in dataloader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                test_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        accuracy = 100. * correct / total
        avg_loss = test_loss / len(dataloader)
        return avg_loss, accuracy
    
    def train(self, train_loader, val_loader, epochs=50):
        """完整训练流程"""
        train_losses, train_accs = [], []
        val_losses, val_accs = [], []
        
        for epoch in range(epochs):
            print(f'\nEpoch {epoch+1}/{epochs}')
            print('-' * 30)
            
            # 训练
            train_loss, train_acc = self.train_epoch(train_loader)
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            
            # 验证
            val_loss, val_acc = self.validate(val_loader)
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            
            # 调整学习率
            self.scheduler.step(val_loss)
            
            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
            
            # 早停检查
            if len(val_losses) > 10:
                if all(val_losses[-1] >= val_losses[-1-i] for i in range(1, 6)):
                    print("Early stopping triggered")
                    break
        
        return {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'val_losses': val_losses,
            'val_accs': val_accs
        }

# 使用示例
if __name__ == "__main__":
    # 设置设备
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # 创建模型
    model = ConvolutionalNeuralNetwork(num_classes=10, input_channels=1)
    
    # 创建训练器
    trainer = Trainer(model, device)
    
    # 生成示例数据（实际使用中应该是真实数据）
    X_train = torch.randn(1000, 1, 32, 32)
    y_train = torch.randint(0, 10, (1000,))
    X_val = torch.randn(200, 1, 32, 32)
    y_val = torch.randint(0, 10, (200,))
    
    # 创建数据加载器
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # 训练模型
    results = trainer.train(train_loader, val_loader, epochs=10)
    
    print("\n训练完成！")
    print(f"最终验证准确率: {results['val_accs'][-1]:.2f}%")
\end{longcodebox}

\subsection{在codebox中使用minted}

传统的codebox现在也支持跨页显示：

\begin{codebox}[title=数据处理管道实现]
\begin{minted}{python}
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class DataPipeline:
    """
    完整的数据处理管道
    包含数据清洗、特征工程、模型训练和评估
    """
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.feature_names = None
        
    def load_data(self, file_path):
        """加载数据"""
        try:
            if file_path.endswith('.csv'):
                self.data = pd.read_csv(file_path)
            elif file_path.endswith('.xlsx'):
                self.data = pd.read_excel(file_path)
            else:
                raise ValueError("不支持的文件格式")
            
            print(f"数据加载成功: {self.data.shape}")
            return self.data
        except Exception as e:
            print(f"数据加载失败: {e}")
            return None
    
    def explore_data(self):
        """数据探索"""
        if self.data is None:
            print("请先加载数据")
            return
        
        print("=== 数据基本信息 ===")
        print(f"数据形状: {self.data.shape}")
        print(f"列名: {list(self.data.columns)}")
        print("\n=== 数据类型 ===")
        print(self.data.dtypes)
        
        print("\n=== 缺失值统计 ===")
        missing_data = self.data.isnull().sum()
        missing_percent = 100 * missing_data / len(self.data)
        missing_table = pd.DataFrame({
            '缺失数量': missing_data,
            '缺失比例': missing_percent
        })
        print(missing_table[missing_table['缺失数量'] > 0])
        
        print("\n=== 数值型变量统计 ===")
        numeric_columns = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) > 0:
            print(self.data[numeric_columns].describe())
        
        print("\n=== 分类变量统计 ===")
        categorical_columns = self.data.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            print(f"\n{col} 的取值分布:")
            print(self.data[col].value_counts().head())
    
    def clean_data(self):
        """数据清洗"""
        if self.data is None:
            print("请先加载数据")
            return
        
        print("开始数据清洗...")
        original_shape = self.data.shape
        
        # 删除重复行
        self.data = self.data.drop_duplicates()
        print(f"删除重复行: {original_shape[0] - self.data.shape[0]} 行")
        
        # 处理缺失值
        numeric_columns = self.data.select_dtypes(include=[np.number]).columns
        categorical_columns = self.data.select_dtypes(include=['object']).columns
        
        # 数值型变量用中位数填充
        for col in numeric_columns:
            if self.data[col].isnull().sum() > 0:
                median_value = self.data[col].median()
                self.data[col].fillna(median_value, inplace=True)
                print(f"{col}: 用中位数 {median_value:.2f} 填充缺失值")
        
        # 分类变量用众数填充
        for col in categorical_columns:
            if self.data[col].isnull().sum() > 0:
                mode_value = self.data[col].mode()[0]
                self.data[col].fillna(mode_value, inplace=True)
                print(f"{col}: 用众数 '{mode_value}' 填充缺失值")
        
        # 处理异常值（使用IQR方法）
        for col in numeric_columns:
            Q1 = self.data[col].quantile(0.25)
            Q3 = self.data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = self.data[(self.data[col] < lower_bound) | 
                               (self.data[col] > upper_bound)]
            if len(outliers) > 0:
                print(f"{col}: 发现 {len(outliers)} 个异常值")
                # 将异常值替换为边界值
                self.data.loc[self.data[col] < lower_bound, col] = lower_bound
                self.data.loc[self.data[col] > upper_bound, col] = upper_bound
        
        print(f"数据清洗完成，最终数据形状: {self.data.shape}")
    
    def feature_engineering(self, target_column):
        """特征工程"""
        if self.data is None:
            print("请先加载数据")
            return
        
        print("开始特征工程...")
        
        # 分离特征和目标变量
        if target_column not in self.data.columns:
            print(f"目标变量 '{target_column}' 不存在")
            return
        
        X = self.data.drop(columns=[target_column])
        y = self.data[target_column]
        
        # 处理分类变量
        categorical_columns = X.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col])
            self.label_encoders[col] = le
            print(f"对 {col} 进行标签编码")
        
        # 保存特征名称
        self.feature_names = list(X.columns)
        
        # 标准化数值特征
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
        
        print(f"特征工程完成，特征数量: {X_scaled.shape[1]}")
        return X_scaled, y
    
    def train_model(self, X, y, test_size=0.2):
        """训练模型"""
        print("开始模型训练...")
        
        # 分割数据
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # 训练模型
        self.model.fit(X_train, y_train)
        
        # 预测
        y_pred = self.model.predict(X_test)
        
        # 评估模型
        print("\n=== 模型评估结果 ===")
        print(classification_report(y_test, y_pred))
        
        # 特征重要性
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\n=== 特征重要性 TOP 10 ===")
        print(feature_importance.head(10))
        
        # 可视化
        self.visualize_results(y_test, y_pred, feature_importance)
        
        return {
            'X_train': X_train, 'X_test': X_test,
            'y_train': y_train, 'y_test': y_test,
            'y_pred': y_pred, 'feature_importance': feature_importance
        }
    
    def visualize_results(self, y_test, y_pred, feature_importance):
        """结果可视化"""
        plt.figure(figsize=(15, 10))
        
        # 混淆矩阵
        plt.subplot(2, 3, 1)
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('混淆矩阵')
        plt.ylabel('真实值')
        plt.xlabel('预测值')
        
        # 特征重要性
        plt.subplot(2, 3, 2)
        top_features = feature_importance.head(10)
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.title('特征重要性 TOP 10')
        plt.xlabel('重要性')
        
        # 预测概率分布
        plt.subplot(2, 3, 3)
        if hasattr(self.model, 'predict_proba'):
            proba = self.model.predict_proba(self.scaler.transform(
                self.data.drop(columns=[self.data.columns[-1]]).select_dtypes(include=[np.number])
            ))
            plt.hist(proba[:, 1], bins=30, alpha=0.7, edgecolor='black')
            plt.title('预测概率分布')
            plt.xlabel('正类概率')
            plt.ylabel('频率')
        
        plt.tight_layout()
        plt.show()
    
    def run_pipeline(self, file_path, target_column):
        """运行完整管道"""
        print("=== 开始数据处理管道 ===")
        
        # 加载数据
        if self.load_data(file_path) is None:
            return
        
        # 数据探索
        self.explore_data()
        
        # 数据清洗
        self.clean_data()
        
        # 特征工程
        X, y = self.feature_engineering(target_column)
        if X is None:
            return
        
        # 模型训练
        results = self.train_model(X, y)
        
        print("\n=== 管道执行完成 ===")
        return results

# 使用示例
if __name__ == "__main__":
    # 创建管道实例
    pipeline = DataPipeline()
    
    # 运行管道（需要提供实际的数据文件路径和目标列名）
    # results = pipeline.run_pipeline('your_data.csv', 'target_column')
    
    print("数据处理管道已就绪")
    print("使用方法: pipeline.run_pipeline('数据文件路径', '目标列名')")
\end{minted}
\end{codebox} 